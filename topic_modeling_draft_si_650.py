# -*- coding: utf-8 -*-
"""Topic Modeling Draft SI 650.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAQk5LmQs42IyMMm5F-EDiak3sq9wqxw

NOTE: This is a skeleton, we still need to adapt this into our application framework.
"""

# Commented out IPython magic to ensure Python compatibility.
#jww comment  

import pandas as pd
import numpy as np
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
import plotly
import matplotlib as plt
import nltk
# %matplotlib inline

import os.path
from os import path

#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')

"""**1. Data Pre-Processing**"""

def print_top_words(model, feature_names, n_top_words):
    my_return = {}
    for topic_idx, topic in enumerate(model.components_):
        print("Topic #%d:" % topic_idx)
        my_value = " ".join([feature_names[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(my_value)
        my_return[topic_idx] = my_value
    print()
    return my_return

#import tweets into dataframe (or list if we are not doing a visual component)
def get_topic_models(df, n_top_words):

#    name = "data/{}.json".format(file_id)

#    df = pd.read_json(name, orient='records')

#    print(df.head(10))
    # convert tweets into list
    text = df

    # remove words with hashtags in front? remove just the hashtag? remove words with @ in front?

    from nltk.stem import WordNetLemmatizer
    lemm = WordNetLemmatizer()

    #borrowed from another project, so if this doesn't work I can build out a simpler step by step process
    class LemmaCountVectorizer(CountVectorizer):
        def build_analyzer(self):
            analyzer = super(LemmaCountVectorizer, self).build_analyzer()
            return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))

    tf_vectorizer = LemmaCountVectorizer(max_df=0.95, 
                                        min_df=2,
                                        stop_words='english',
                                        decode_error='ignore')
    tf = tf_vectorizer.fit_transform(text)



    """**2. LDA**"""

    lda = LatentDirichletAllocation(n_components=5, max_iter=5,
                                    learning_method = 'online',
                                    learning_offset = 50.,
                                    random_state = 0)

    lda.fit(tf)

    print("\nTopics in These Tweets: ")
    tf_feature_names = tf_vectorizer.get_feature_names() # grabbing the words from a tf-idf vector
    return print_top_words(lda, tf_feature_names, n_top_words) # prints out top 40 words from each 'topic' cluster

    """**3. Topic Plots for More Visual Info?**

    *   We could also do wordclouds if that's more interesting to y'all
    """

    import seaborn as sns

    doc_topic = lda.fit_transform(tf)

    topic_list = []
    for n in range(doc_topic.shape[0]):
        topic_most_pr = doc_topic[n].argmax()
        topic_list.append(topic_most_pr)

    df['topic'] = topic_list

    tweets = df['text']
    ax = sns.countplot(x='topic', data = df)
    plt.pyplot.show() # show what proportions the topics are represented at

    """**Next Steps:**


    *   Front & Back End Set Up
    *   Integrate Model into App
    *   Test with Data and Debug --> See prof
    *   Evaluate and possibly pivot

    **by Thursday**

    * modelling code will be in place tonight
    * Priya will finish the modeling code in github
    """

