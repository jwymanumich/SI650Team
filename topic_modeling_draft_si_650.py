# -*- coding: utf-8 -*-
"""Topic Modeling Draft SI 650.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xAQk5LmQs42IyMMm5F-EDiak3sq9wqxw

NOTE: This is a skeleton, we still need to adapt this into our application framework.
"""

# Commented out IPython magic to ensure Python compatibility.
#jww comment  

import pandas as pd
import numpy as np
import sklearn
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import NMF, LatentDirichletAllocation
import plotly
import matplotlib as plt
import nltk
from wordcloud import WordCloud, STOPWORDS
# %matplotlib inline

import os.path
from os import path

#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')

"""**1. Data Pre-Processing**"""

def print_top_words(model, feature_names, n_top_words):
    my_return = {}
    for topic_idx, topic in enumerate(model.components_):
        print("Topic #%d:" % topic_idx)
        my_value = " ".join([feature_names[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]])
        print(my_value)
        my_return[topic_idx] = my_value
    print()
    return my_return

#import tweets into dataframe (or list if we are not doing a visual component)
def get_topic_models(df, n_top_words):

#    name = "data/{}.json".format(file_id)

#    df = pd.read_json(name, orient='records')

#    print(df.head(10))
    # convert tweets into list
    text = df

    # remove words with hashtags in front? remove just the hashtag? remove words with @ in front?

    from nltk.stem import WordNetLemmatizer
    lemm = WordNetLemmatizer()

    #borrowed from another project, so if this doesn't work I can build out a simpler step by step process
    class LemmaCountVectorizer(CountVectorizer):
        def build_analyzer(self):
            analyzer = super(LemmaCountVectorizer, self).build_analyzer()
            return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))

    tf_vectorizer = LemmaCountVectorizer(max_df=0.95, 
                                        min_df=2,
                                        stop_words='english',
                                        decode_error='ignore')
    tf = tf_vectorizer.fit_transform(text)



    """**2. LDA**"""

    lda = LatentDirichletAllocation(n_components=5, max_iter=5,
                                    learning_method = 'online',
                                    learning_offset = 50.,
                                    random_state = 0)

    lda.fit(tf)
    
    n_top_words = 40
    print("\nTopics in These Tweets: ")
    tf_feature_names = tf_vectorizer.get_feature_names() # grabbing the words from a tf-idf vector
    return print_top_words(lda, tf_feature_names, n_top_words) # prints out top 40 words from each 'topic' cluster

    """**3. Topic Plots for More Visual Info?** """

    import seaborn as sns

    doc_topic = lda.fit_transform(tf)

    topic_list = []
    for n in range(doc_topic.shape[0]):
        topic_most_pr = doc_topic[n].argmax()
        topic_list.append(topic_most_pr)

    df['topic'] = topic_list

    tweets = df['text']
    ax = sns.countplot(x='topic', data = df)
    plt.pyplot.show() # show what proportions the topics are represented at
    
     """**3. Word Clouds for More Visual Info?** """
        
     
    first_topic = lda.components_[0]
    second_topic = lda.components_[1]
    third_topic = lda.components_[2]
    fourth_topic = lda.components_[3]
    fifth_topic = lda.components_[4]
    
    first_topic_words = [tf_feature_names[i] for i in first_topic.argsort()[:-50 - 1 :-1]]
    second_topic_words = [tf_feature_names[i] for i in second_topic.argsort()[:-50 - 1 :-1]]
    third_topic_words = [tf_feature_names[i] for i in third_topic.argsort()[:-50 - 1 :-1]]
    fourth_topic_words = [tf_feature_names[i] for i in fourth_topic.argsort()[:-50 - 1 :-1]]
    fifth_topic_words = [tf_feature_names[i] for i in fifth_topic.argsort()[:-50 - 1 :-1]]
    
    # Generating the wordcloud with the values under the category dataframe
    firstcloud = WordCloud(
                              stopwords=STOPWORDS,
                              background_color='black',
                              width=5000,
                              height=3600
                             ).generate(" ".join(first_topic_words))
    plt.pyplot.imshow(firstcloud)
    plt.pyplot.axis('off')
    plt.pyplot.show()
    
    #second cloud
    cloud = WordCloud(
                              stopwords=STOPWORDS,
                              background_color='black',
                              width=2500,
                              height=1800
                             ).generate(" ".join(second_topic_words))
    plt.pyplot.imshow(cloud)
    plt.pyplot.axis('off')
    plt.pyplot.show()
    
    #third cloud
    cloud = WordCloud(
                              stopwords=STOPWORDS,
                              background_color='black',
                              width=2500,
                              height=1800
                             ).generate(" ".join(third_topic_words))
    plt.pyplot.imshow(cloud)
    plt.pyplot.axis('off')
    plt.pyplot.show()
    
    #fourth cloud
    cloud = WordCloud(
                              stopwords=STOPWORDS,
                              background_color='black',
                              width=2500,
                              height=1800
                             ).generate(" ".join(fourth_topic_words))
    plt.pyplot.imshow(cloud)
    plt.pyplot.axis('off')
    plt.pyplot.show()
    
    #fifth cloud
    cloud = WordCloud(
                              stopwords=STOPWORDS,
                              background_color='black',
                              width=2500,
                              height=1800
                             ).generate(" ".join(fifth_topic_words))
    plt.pyplot.imshow(cloud)
    plt.pyplot.axis('off')
    plt.pyplot.show()

    """**Next Steps:**


    *   Front & Back End Set Up
    *   maybe add sample tweets for each topic? 
    *   Integrate Model into App
    *   Test with Data and Debug --> See prof
    *   Evaluate

    **by Thursday**

    * modelling code will be in place tonight
    * Priya will finish the modeling code in github
    """

